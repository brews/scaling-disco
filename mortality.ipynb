{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install muuttaa==0.1.0 metacsv\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import uuid\n",
    "import csv\n",
    "import re\n",
    "from os import PathLike\n",
    "from io import BufferedIOBase\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "from dask_gateway import GatewayCluster\n",
    "import fsspec\n",
    "import metacsv\n",
    "import muuttaa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from xhistogram.xarray import histogram\n",
    "from xclim.core import units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUPYTER_IMAGE = os.environ.get(\"JUPYTER_IMAGE\")\n",
    "UID = str(uuid.uuid4())\n",
    "START_TIME = datetime.datetime.now(datetime.timezone.utc).isoformat()\n",
    "\n",
    "print(\n",
    "    f\"\"\"\n",
    "        {JUPYTER_IMAGE=}\n",
    "        {START_TIME=}\n",
    "        {UID=}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "CMIP_URI = \"gs://new_carb_demo/clean_cmip5.zarr\"\n",
    "SOCIOECONOMICS_URI = \"gs://new_carb_demo/clean_socioeconomics.zarr\"\n",
    "CMIP5_SEGMENT_WEIGHTS_URI = \"gs://new_carb_demo/clean_cmip5_segment_weights.zarr\"\n",
    "GAMMA_URI = \"gs://new_carb_demo/clean_gamma.zarr\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up models and regionalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformation and regionalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _no_processing(ds: xr.Dataset) -> xr.Dataset:\n",
    "    return ds\n",
    "\n",
    "\n",
    "def _make_annual_tas(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Compute annual average for 'tas'.\n",
    "    \"\"\"\n",
    "    return ds[[\"tas\"]].groupby(\"time.year\").mean(\"time\")\n",
    "\n",
    "\n",
    "def _make_30hbartlett_climtas(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    From annaual 'tas' compute 30-year half-Bartlett kernel average in degC.\n",
    "\n",
    "    Output variable is \"climtas\". This assumes input's \"tas\" has \"year\"\n",
    "    time dim.\n",
    "    \"\"\"\n",
    "    kernel_length = 30\n",
    "    w = np.arange(kernel_length)\n",
    "    weight = xr.DataArray(w / w.sum(), dims=[\"window\"])\n",
    "    da = ds[\"tas\"].rolling(year=30).construct(year=\"window\").dot(weight)\n",
    "    # TODO: What to do for NaNs? What happened in carb analysis for climtas? Check 'gs://rhg-data/climate/aggregated/NASA/NEX-GDDP-BCSD-reformatted/California_2019_census_tracts_weighted_by_population/{scenario}/{model}/tas-bartlett30/tas-bartlett30_BCSD_CA-censustract2019_{model}_{scenario}_{version}_{year}.zarr'\n",
    "    return da.to_dataset(name=\"climtas\").astype(\"float32\")\n",
    "\n",
    "\n",
    "make_climtas = muuttaa.TransformationStrategy(\n",
    "    preprocess=_make_annual_tas,\n",
    "    postprocess=_make_30hbartlett_climtas,\n",
    ")\n",
    "\n",
    "\n",
    "def _make_tas_20yrmean_annual_histogram(ds: xr.Dataset) -> xr.Dataset:\n",
    "    bins = np.arange(230, 341)  # Range we get histogram count for. NOTE: in Kelvin!\n",
    "    tas_annual_histogram = (\n",
    "        ds[\"tas\"].groupby(\"time.year\").map(histogram, bins=[bins], dim=[\"time\"])\n",
    "    )\n",
    "\n",
    "    tas_histogram_20yr = (\n",
    "        tas_annual_histogram.rolling(year=20, center=True).mean().to_dataset()\n",
    "    )\n",
    "    return tas_histogram_20yr.astype(\"float32\")\n",
    "\n",
    "\n",
    "make_tas_20yrmean_annual_histogram = muuttaa.TransformationStrategy(\n",
    "    preprocess=_make_tas_20yrmean_annual_histogram,\n",
    "    postprocess=_no_processing,\n",
    ")\n",
    "\n",
    "# TODO: Where to use `da = units.convert_units_to(da, \"degC\")`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact projection model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the impact projection model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mortality_impact_model(ds: xr.Dataset) -> xr.Dataset:\n",
    "    # dot product of betas and t_bins * census tract age-spec populations\n",
    "    _effect = (ds[\"histogram_tas\"] * ds[\"beta\"]).sum(dim=\"tas_bin\") * ds[\"share\"]\n",
    "\n",
    "    # impacts are difference of future - historical effect\n",
    "    impact = _effect.sel(year=2050) - _effect.sel(year=2020)\n",
    "\n",
    "    return xr.Dataset({\"impact\": impact, \"_effect\": _effect})\n",
    "\n",
    "\n",
    "# If you already have beta.\n",
    "mortality_impact_model = muuttaa.Projector(\n",
    "    preprocess=_no_processing,\n",
    "    project=_mortality_impact_model,\n",
    "    postprocess=_no_processing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...but here is an additional model with logic about how to get \"betas\" from the input gammas..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uclip(da, dim, lmmt=10, ummt=30):\n",
    "    \"\"\"Performs U Clipping of an unclipped response function for all regions\n",
    "    simultaneously, centered around each region's Minimum Mortality Temperature (MMT).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    da : DataArray\n",
    "        xarray.DataArray of unclipped response functions\n",
    "    dim : str\n",
    "        Dimension name along which clipping will be applied. Clipping will be applied\n",
    "        along dimensions `dim` for all other dimensions independently (e.g. a unique\n",
    "        minimum point will be found for each combination of any other dimensions in the\n",
    "        data).\n",
    "    lmmt : int\n",
    "        Lower bound of the temperature range in which a minimum mortality temperature\n",
    "        will be searched for. Default is 10.\n",
    "    ummt : int\n",
    "        Upper bound of the temperature range in which a minimum mortality temperature\n",
    "        will be searched for. Default is 30.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    clipped : xarray DataArray of the regions response functioned centered on its mmt\n",
    "    \"\"\"\n",
    "    # identify mmt within defined range\n",
    "    range_msk = (da[dim] >= lmmt) & (da[dim] <= ummt)\n",
    "    min_idx = da.where(range_msk).idxmin(dim=dim)\n",
    "    min_val = da.where(range_msk).min(dim=dim)\n",
    "\n",
    "    # subtract mmt beta value\n",
    "    diffed = da - min_val\n",
    "\n",
    "    # mask data on each side of mmt and take cumulative max in each direction\n",
    "    rhs = (\n",
    "        diffed.where(diffed[dim] >= min_idx)\n",
    "        .rolling({dim: len(diffed[dim])}, min_periods=1)\n",
    "        .max()\n",
    "    )\n",
    "    lhs = (\n",
    "        diffed.where(diffed[dim] <= min_idx)\n",
    "        .sel({dim: slice(None, None, -1)})\n",
    "        .rolling({dim: len(diffed[dim])}, min_periods=1)\n",
    "        .max()\n",
    "        .sel({dim: slice(None, None, -1)})\n",
    "    )\n",
    "\n",
    "    # combine the arrays where they've been masked\n",
    "    clipped = rhs.fillna(lhs)\n",
    "    return clipped\n",
    "\n",
    "\n",
    "def _add_degree_coord(da: xr.DataArray, max_degrees: int | float) -> xr.DataArray:\n",
    "    \"\"\"\n",
    "    Raises array to 1 ... max_degrees power, concatenating all together in a new \"degree\" coordinate.\n",
    "    \"\"\"\n",
    "    if max_degrees < 2:\n",
    "        # TODO: Test what actually happens for this edge case.\n",
    "        # Raising an error because we're avoiding calculating da^1 because da is sometimes really big,\n",
    "        # not sure the code handles this case very well and it's likely a mistake so just raising an\n",
    "        # error for now.\n",
    "        raise ValueError(\"'max_degree' arg must be >= 2\")\n",
    "\n",
    "    degree_idx = list(range(1, max_degrees + 1))\n",
    "    out = xr.concat(\n",
    "        [da]\n",
    "        + [\n",
    "            da**i for i in degree_idx[1:]\n",
    "        ],  # Avoids computing ds^1 to not add tasks to dask graph when dask-backed data.\n",
    "        pd.Index(degree_idx, name=\"degree\"),\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "def _beta_from_gamma(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"\n",
    "    Calculates mortality impact polynomial model's beta coefficients from gamma coefficients.\n",
    "\n",
    "    Returns a copy of `ds` with new \"beta\" variable.\n",
    "    \"\"\"\n",
    "    # The ds[\"gamma\"] has a \"covarname\" dimension with one element for each of the model's covariates.\n",
    "    # This unpacks \"covarname\" so each covariate gamma coefficient is its own variable.\n",
    "    # Doing this to try to make math easier to read.\n",
    "    gamma_1 = ds[\"gamma\"].sel(\n",
    "        covarname=\"1\"\n",
    "    )  # coefficient for predictor tas (tas histogram bin labels).\n",
    "    gamma_climtas = ds[\"gamma\"].sel(\n",
    "        covarname=\"climtas\"\n",
    "    )  # coefficient for climtas covariate.\n",
    "    gamma_loggdppc = ds[\"gamma\"].sel(\n",
    "        covarname=\"loggdppc\"\n",
    "    )  # coefficient log of GDP per capita covariate.\n",
    "\n",
    "    # Remember, annual histograms as input use histogram bin labels (\"tas_bin\") as \"tas\".\n",
    "    # Creates a \"degree\" coordinate and populates it with tas^1, tas^2, tas^3, etc. equal to degrees in polynomial.\n",
    "    tas = _add_degree_coord(ds[\"tas_bin\"], max_degrees=gamma_1[\"degree\"].size)\n",
    "    # Do it this way so we don't need to repeat the same math for each degree of the polynomial below.\n",
    "\n",
    "    #  γ_1 * tas + γ_climtas * climtas * tas + γ_loggdppc * loggdppc * tas\n",
    "    # term for each of the polynomial degrees (∵ \"degree\" is a coordinate for variables that vary by degree).\n",
    "    beta = (\n",
    "        gamma_1 * tas\n",
    "        + gamma_climtas * ds[\"climtas\"] * tas\n",
    "        + gamma_loggdppc * ds[\"loggdppc\"] * tas\n",
    "    ).sum(\"degree\")  # Sum together terms for all degrees of polynomial.\n",
    "\n",
    "    # Uclip beta across tas histogram's bin labels, so basically across range of daily temperature values.\n",
    "    beta = uclip(beta, dim=\"tas_bin\")\n",
    "    # Returns new dataset with beta added as new variable. Not modifying\n",
    "    # original ds. Also ensure original data is passed through to projection.\n",
    "    return ds.assign({\"beta\": beta})\n",
    "\n",
    "\n",
    "# If you have gamma and need to compute beta.\n",
    "mortality_impact_model_gamma = muuttaa.Projector(\n",
    "    preprocess=_beta_from_gamma,  # Not sure this should actually be a preprocess but I'm lazy.\n",
    "    project=_mortality_impact_model,\n",
    "    postprocess=_no_processing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Valuation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mortality_valuation_model(ds: xr.Dataset) -> xr.Dataset:\n",
    "    # Total damages are age-spec physical impacts (deaths/100k) * age-spec population * scale * vsl\n",
    "    damages_total = ds[\"impact\"] * ds[\"pop\"] * ds[\"scale\"] * ds[\"vsl\"]\n",
    "\n",
    "    # Damages per capita = total damages / population\n",
    "    damages_pc = ds[\"impact\"] * ds[\"scale\"] * ds[\"vsl\"]\n",
    "\n",
    "    # Damages as share of average tract income = damages per capita / income per capita\n",
    "    damages_pincome = damages_pc / ds[\"pci\"]\n",
    "\n",
    "    out = xr.Dataset(\n",
    "        {\n",
    "            \"damages_total\": damages_total,\n",
    "            \"damages_pc\": damages_pc,\n",
    "            \"damages_pincome\": damages_pincome,\n",
    "        }\n",
    "    )\n",
    "    return out\n",
    "\n",
    "\n",
    "mortality_valuation_model = muuttaa.Projector(\n",
    "    preprocess=_no_processing,\n",
    "    project=_mortality_valuation_model,\n",
    "    postprocess=_no_processing,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing to run the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that we need for transformation and regionalization are the weights to regionalize the gridded climate fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segment_weights = muuttaa.SegmentWeights(xr.open_zarr(CMIP5_SEGMENT_WEIGHTS_URI))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in socioeconomic input data and grab values needed for the impact model. Because we're calculating the impact model beta parameter from gamma, we also want to grab the gamma draws we're interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "socioeconomics = xr.open_zarr(SOCIOECONOMICS_URI)\n",
    "gammas = xr.open_zarr(GAMMA_URI)\n",
    "\n",
    "\n",
    "impact_params = xr.Dataset(\n",
    "    {\n",
    "        \"loggdppc\": socioeconomics[\"loggdppc\"],\n",
    "        \"gamma\": gammas[\"gamma_mean\"],\n",
    "        #\"gamma\": gammas[\"gamma_sampled\"],  # Run with MVN sampled gammas\n",
    "        \"share\": socioeconomics[\"pop_share\"]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the valuation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set VSL at EPA value of $7.4 M in 2006$ converted to 2019$\n",
    "vsl = 9146910\n",
    "vsl2 = 9926525\n",
    "\n",
    "# convert to impacts per person\n",
    "scale = 1 / 100000\n",
    "# NOTE: Beware we're multiplying big numbers by small numbers here.\n",
    "\n",
    "valuation_params = xr.Dataset(\n",
    "    {\n",
    "        \"vsl\": vsl2,\n",
    "        \"scale\": scale,\n",
    "        \"pci\": socioeconomics[\"pci\"],\n",
    "        \"pop\": socioeconomics[\"pop\"],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is one way to run a single ensemble member. This is a useful test case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a single test dataset.\n",
    "# TODO: We really should rechunk this as part of cleaning.\n",
    "cmip = xr.open_datatree(CMIP_URI, engine=\"zarr\", chunks={}).chunk(\n",
    "    {\"time\": 365 * 20, \"lat\": 90, \"lon\": 90}\n",
    ")\n",
    "\n",
    "test_ds = cmip[\"rcp45/ACCESS1-0\"].ds\n",
    "\n",
    "# Need dask cluster for these climate transformations.\n",
    "with GatewayCluster(\n",
    "        worker_image=JUPYTER_IMAGE, scheduler_image=JUPYTER_IMAGE, profile=\"micro\"\n",
    ") as cluster:\n",
    "    client = cluster.get_client()\n",
    "    print(client.dashboard_link)\n",
    "    cluster.scale(50)\n",
    "\n",
    "    transformed = muuttaa.apply_transformations(\n",
    "        test_ds,\n",
    "        regionalize=segment_weights,\n",
    "        strategies=[\n",
    "            make_climtas,\n",
    "            make_tas_20yrmean_annual_histogram,\n",
    "        ],\n",
    "    )\n",
    "    transformed = transformed.compute()\n",
    "\n",
    "transformed = (\n",
    "    transformed\n",
    "    .assign_coords(tas_bin=(transformed[\"tas_bin\"] - 273))\n",
    "    .assign(climtas=(transformed[\"climtas\"] - 273))\n",
    "    .sel(year=slice(1990, 2098))  # Years outside this have NA due to climtas rolling operations.\n",
    ")\n",
    "\n",
    "mortality_impacts = muuttaa.project(\n",
    "    transformed,\n",
    "    model=mortality_impact_model_gamma,\n",
    "    parameters=impact_params,\n",
    ")\n",
    "mortality_impacts = mortality_impacts.compute()\n",
    "\n",
    "# Usually don't need dask cluster to value these damages.\n",
    "mortality_damages = muuttaa.project(\n",
    "    mortality_impacts,\n",
    "    model=mortality_valuation_model,\n",
    "    parameters=valuation_params,\n",
    ")\n",
    "mortality_damages = mortality_damages.compute()\n",
    "print(mortality_damages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the entire ensemble with only damages output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the entire ensemble through each test, dumping intermediate results to scratch.\n",
    "cmip = xr.open_datatree(CMIP_URI, engine=\"zarr\", chunks={})\n",
    "\n",
    "\n",
    "def _project_damages(ds):\n",
    "    transformed = muuttaa.apply_transformations(\n",
    "        ds,\n",
    "        regionalize=segment_weights,\n",
    "        strategies=[\n",
    "            make_climtas,\n",
    "            make_tas_20yrmean_annual_histogram,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    impacts = muuttaa.project(\n",
    "        transformed,\n",
    "        model=mortality_impact_model_gamma,\n",
    "        parameters=impact_gamma_params,\n",
    "        merge_predictors_parameters=_merge_impact_inputs,\n",
    "    )\n",
    "\n",
    "    damages = muuttaa.project(\n",
    "        impacts,\n",
    "        model=mortality_valuation_model,\n",
    "        parameters=valuation_params,\n",
    "    )\n",
    "\n",
    "    return damages\n",
    "\n",
    "\n",
    "with GatewayCluster(worker_image=JUPYTER_IMAGE, scheduler_image=JUPYTER_IMAGE, profile=\"micro\") as cluster:\n",
    "    client = cluster.get_client()\n",
    "    print(client.dashboard_link)\n",
    "    cluster.scale(50)\n",
    "\n",
    "    damages = cmip.map_over_datasets(_project_damages).compute()\n",
    "\n",
    "\n",
    "print(damages)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
